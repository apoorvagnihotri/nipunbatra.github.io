\documentclass[colorlinks,linkcolor=true]{article}
\usepackage{hyperref,verbatim}
\usepackage[usenames, dvipsnames]{color}
\usepackage{amsmath}
\usepackage{tgpagella} % text only
\usepackage{mathpazo} 
\usepackage[margin=0.6in]{geometry}
\addtolength{\topmargin}{0in}


\newcommand{\deadline}{Midnight Feb 5 }
\newcommand{\extdeadline}{Midnight Feb 7}
\newcommand{\total}{20}

%opening
\title{Machine Learning\\Homework 3 : Regression - I\\(due \deadline)}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\noindent\fbox{
		\parbox{\textwidth}{
			Instructions\\
			\begin{enumerate}
				\item In case you are unfamiliar with the Python data ecosystem (NumPy, Pandas), you are recommended to study the first four chapters of the \href{https://jakevdp.github.io/PythonDataScienceHandbook/}{Python data science handbook}. A doubt clearing session would be organised in case you have any difficulties in the data science ecosystem.
				\item The deadline for full score is \deadline. You can get 50\% credit for late submission (\extdeadline).
				\item Total marks = \total
				\item You have to type the assignment using a word processing engine, create a pdf and upload on the form. Please note that only pdf files will be accepted.
				\item All code/Jupyter notebooks must be put up as \href{https://gist.github.com/}{\textbf{secret gists}} and linked in the created pdf submission. Again, only secret gists. Not public ones.
				\item Any instances of cheating/plagiarism will not be tolerated at all. 
				\item Cite all the pertinent references in IEEE format.
				\item The least count of grading would be 0.5 marks. 
				
			\end{enumerate}
		}
	}


\begin{enumerate}
	
	\item Learn $y = \theta_0 + \theta_1\times x$ on the following small dataset on pen and paper. You may scan or click picture of your answers and attach to the pdf.
	$$X = \begin{bmatrix}
	
	
	1       \\
	3      \\
	6      \\

	
	\end{bmatrix}$$
	and 
	
	$$Y = \begin{bmatrix}
	
	
	6 \\
	10  \\
	16  \\
	
	\end{bmatrix}$$
	using:
	\begin{enumerate}
		\item Normal equations or matrix method \textbf{[1 mark]}
		\item Gradient desent where initial values of $\theta_0, \theta_1)$ is (0, 0) and step size (or learning rate) is $\alpha=0.1$. Show the calculations for initial 5 iterations. \textbf{[1 mark]}
		\item Using the formula in terms of covariance and variance \textbf{[1 mark]}
		
	\end{enumerate}



	\item 	
	\begin{enumerate}
		\item For the following X and y, use scikit-learn to learn a linear model. \textbf{[1 mark]}
		\item Solve the problem using normal equations. You may find that one of the matrix in the normal equation is non-invertible. Why does the matrix turn out to be non-invertible? Why can scikit-learn implementation still correctly solve this regression problem?  \textbf{[1 mark]}
	\end{enumerate}
	
$$X = \begin{bmatrix}


1       & 2 \\
2      &  4  \\
3      &  6  \\
4      &  8  \\

\end{bmatrix}$$
and 

$$Y = \begin{bmatrix}


2 \\
3  \\
4  \\
5  \\

\end{bmatrix}$$



\item  \begin{enumerate}
	

\item Show the usage of scikit learn's linear regression module for the real estate price prediction \href{https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set}{regression problem}.  What is the RMS error on the test set?  \textbf{[1 mark]}

\item Based on the regression co-efficients what can you comment about the importance of different features? Is it correct to assume that larger co-efficients means more important feature?  \textbf{[1 mark]}
\item Now, standardize the dataset to have all features on a scale of 0 to 1. Re-learn the regression co-efficients and now comment on the importance of different features.  \textbf{[1 mark]}

\item Use cross-validation to find the optimal set of features to use for regression. 
\begin{enumerate}
	\item Using all possible feature sets of length 1, 2, 3, or 4, what is the optimal feature set as per the validation set and how does this set of features perform on the test set wrt the model learnt on the entire feature set?  \textbf{[1 mark]}
	\item Use Sequential Forward Selection (or Stepwise Forward Selection) which is a greedy procedure to find the optimal set of features. How does this how does it perform on the test set wrt the model learnt on the entire feature set?  \textbf{[1 mark]}
\end{enumerate}
\end{enumerate}

\item In this question, you will be writing your custom linear regression implementation. 
\begin{enumerate}
	\item Write a function \texttt{normalEquationRegression(X, y)} where $X$ is our feature matrix containing $N$ samples (rows) and $d$ features (columns) and $y$ is our output vector containing $N$ samples. This function returns a vector $\theta$ containing $d+1$ rows. You are free to use numpy's matrix inverse, determinant and multiplication routines. \textbf{[1 mark]}
	\item Write a function \texttt{gradientDescentRegression(X, y, alpha = 0.1)} to learn the regression coefficients using gradient descent. You have to write the formulae for gradient wrt the different $\theta_j \forall j \in (1, ..d)$ \textbf{[1 mark]}
	\item Write a function \texttt{gradientDescentAutogradRegression(X, y, alpha = 0.1)} to learn the regression coefficients using gradient descent. Instead of writing the formulae for computing gradients by yourself, you will use \href{https://github.com/HIPS/autograd}{Autograd} to automatically do that for you. \textbf{[1 mark]}
	\item Write a function \texttt{gradientDescentPyTorchRegression(X, y, alpha = 0.1)} to learn the regression coefficients using gradient descent. Instead of numpy, you would now be using PyTorch. This question will set the ground for your project and future assignment. Similar to Autograd linked in the previous question, PyTorch also has an automatic gradient computation routine that you should make use of. Please note that do not use \texttt{nn.Linear()} for this question. \textbf{[1 mark]}
	\item Illustrate the usage of all of your above four versions on the real estate price dataset in Q3. Report the time taken and accuracy for the four implementations compared with scikit-learn inbuilt function. Use all features from the dataset for all the methods. \textbf{[2 marks]}
\end{enumerate}

\item In this question, we will be implementing polynomial regression as a special case of linear regression. First, we will be generating some data. 

\begin{verbatim}
import numpy as np
x = np.arange(0, 20.1, 0.1)
np.random.seed(0)
y = 1*x**5 + 3*x**4 - 100*x**3 + 8*x**2 -300*x - 1e5 + np.random.randn(len(x))*1e5
\end{verbatim}

Now, we want to learn a polynomial function of degree $p$ on this dataset, i.e. $y = \theta_0 + \theta_1 \times x^1 + \theta_2 \times x^2 + ... \theta_p \times x^p $. We can use our developed linear regression implementations for doing so, by transforming our dataset and creating the matrix X containing columns corresponding to $x^0$, $x^1$, $x^2$, ..., $x^p$.
Using any of your implementations learn the regression coefficients for $p=5$ and $p=4$. How close are your coefficients for $p=5$ to the ones used to generate the data? \textbf{[2 marks]}

\item The following question is to aid our understanding of gradient descent. 
	
	\item What is the time complexity of linear regression? Why?
	\item \begin{enumerate}
		\item Derive an analytical solution for linear regression using $\ell_1$ loss: $|y-X\theta|$. If you cannot solve it analtically (derive a solution similar to the normal equation), explain why? 
		\item Implement a gradient descent based solution. For computing the gradients automatically, use \href{https://github.com/HIPS/autograd}{Autograd}. Create a matplotlib animation to show the learnt line for the following data. The animation should capture the following: i) gradient reducing over time; ii) the fit becoming better over time.
		\item Describe scenarios when you will use $\ell_0$, $\ell_1$ and $\ell_2$ loss.
	\end{enumerate}
	\item \begin{enumerate}
		\item What is the difference between multi-colinearity and co-linearity? 
	

\item \begin{enumerate}
	\item Implement the RANSAC algorithm from scratch. Do not use the inbuilt scikit-learn version. However, it should be a drop-in replacement for scikit-learn's version. Create a Jupyter notebook showing its utility over vanilla Linear regression on the following dataset.
	\item Use your RANSAC implementation to do image stitching as done in this \href{https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/}{post}. You are not expected to understand the details of the post and can directly borrow the code. This exercise is to warm you up to some practical applications of RANSAC.
\end{enumerate}

\item Create a \href{https://ipywidgets.readthedocs.io/en/stable/examples/Widget\%20Basics.html}{Jupyter widget} for depicting the impact of coefficient of regularisation on LASSO and Ridge regression. Use the LASSO and Ridge regression implementations from scikit-learn.

Create a 1X2 subplot matrix in Matplotlib. The columns are for LASSO and Ridge regression and show the linear fit for the two algorithms given the identical dataset. A common regularisation co-efficient ($\lambda$) drives the regularisation in both these cases. Export a GIF recording of the Jupyter widget in action where you vary $\lambda$ on a log-scale from $10^{-4}$ to $10^{4}$. Download the dataset from here.

\item 	Show the usage of scikit learn's linear regression module on Ridge regression for the real estate price prediction \href{https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set}{regression problem}.  Use cross-validation to find the optimum regularisation co-efficient ($\lambda$).




	
	

\end{enumerate}

Some useful references for the homework:

\begin{enumerate}
	\item \href{https://www.youtube.com/watch?v=BpOKB3OzQBQ}{A video on RANSAC algorithm}
\end{enumerate}







\end{enumerate}


\end{document}
